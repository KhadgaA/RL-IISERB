{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7baf3071-61d8-48d1-a9fe-8b05e0096f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "import gym\n",
    "import pygame\n",
    "from gym.spaces import Discrete, Box,Dict\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b012e353-4128-49c3-b1bd-3abe639c9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarehouseAgent():\n",
    "    def __init__(self):\n",
    "        self.GRID_DIM = [7,6]\n",
    "\n",
    "        self.agent_position = [1,2]\n",
    "\n",
    "        self.box_location = [4,3]\n",
    "        self.goal_location = [3,1]\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([-1, 0]),\n",
    "            1: np.array([1, 0]),\n",
    "            2: np.array([0, -1]),\n",
    "            3: np.array([0, 1]),\n",
    "        }\n",
    "        self._ACTIONLOOKUP = {\n",
    "            0: 'move up',\n",
    "            1: 'move down',\n",
    "            2: 'move left',\n",
    "            3: 'move right',\n",
    "            4: 'push'\n",
    "            }\n",
    "        self.GRID_DIM = np.asarray(self.GRID_DIM)\n",
    "        self.GRID = np.zeros(self.GRID_DIM ) # The Boundaries are the walls, so playing space is only [:-2,:-2] \n",
    "        self.GRID[:,[0,-1]] = 1\n",
    "        self.GRID[[0,-1],:] = 1\n",
    "        self.GRID[[1,2,5],3:5] = 1\n",
    "        self.walls = 1\n",
    "        self.action_space = Discrete(len(self._ACTIONLOOKUP.keys()))\n",
    "        self.state_space = Discrete(self.GRID_DIM[0]*self.GRID_DIM[1])\n",
    "        self.observation_space = Dict(\n",
    "            {\n",
    "                \"agent\": Box(np.array([0,0]), np.array([self.GRID_DIM[0]-1,self.GRID_DIM[1] - 1]), shape=(2,), dtype=int),\n",
    "                'box' : Box( np.array([0,0]), np.array([self.GRID_DIM[0]-1,self.GRID_DIM[1] - 1]), shape=(2,), dtype=int),\n",
    "                \"target\": Box( np.array([0,0]), np.array([self.GRID_DIM[0]-1,self.GRID_DIM[1] - 1]), shape=(2,), dtype=int),\n",
    "            })\n",
    "        self._agent_location = np.array(self.agent_position)\n",
    "        self._box_location = np.array(self.box_location)\n",
    "        self._target_location = np.array(self.goal_location) \n",
    "            \n",
    "#         print(self.GRID)\n",
    "     \n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        self._prev_agent_location = None\n",
    "        self._prev_box_location = None\n",
    "        moved_box = False\n",
    "\n",
    "        if action<4:\n",
    "            moved_player = self._move(action)\n",
    "        else:\n",
    "            moved_player, moved_box = self._push(action)\n",
    "            \n",
    "        done, reward = self.is_over()            \n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        \n",
    "        return observation, reward, done, info      \n",
    "        \n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "    \n",
    "    def render(self):\n",
    "        rend = self.GRID.copy().astype(dtype='U1')\n",
    "        rend[self._agent_location[0],self._agent_location[1]] = 'A'\n",
    "        rend[self._box_location[0],self._box_location[1]] = 'B'\n",
    "        rend[self._target_location[0],self._target_location[1]] = 'T'\n",
    "        return rend\n",
    "        \n",
    "\n",
    "    def reset(self,seed = None, return_info = False, options = None):\n",
    "        self._agent_location = np.array(self.agent_position)\n",
    "        self._box_location = np.array(self.box_location)\n",
    "        self._target_location = np.array(self.goal_location)\n",
    "        \n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        return (observation, info) if return_info else observation\n",
    "        \n",
    "    \n",
    "    def _get_obs(self):\n",
    "        return {\"agent\":self._agent_location,\"box\": self._box_location,\"target\":self._target_location}\n",
    "    def _get_info(self):\n",
    "        return {'distance': np.linalg.norm(self._box_location - self._target_location,ord = 1)}\n",
    "    def _push(self,action):\n",
    "        loc = self._box_location - self._agent_location\n",
    "#         print(f'loc{loc}, box :{self._box_location}, agent:{self._agent_location}')\n",
    "        push_dir = None\n",
    "        for idx,val in enumerate(self._action_to_direction.values()):\n",
    "            if np.array_equal(loc,val):\n",
    "                valid = True\n",
    "                push_dir = idx\n",
    "                break\n",
    "            else :\n",
    "                valid = False\n",
    "            \n",
    "        if valid:\n",
    "            self._prev_agent_location = self._agent_location\n",
    "            self._prev_box_location = self._box_location\n",
    "            self._box_location = self._box_location + self._action_to_direction[push_dir]\n",
    "            if self.GRID[self._box_location[0],self._box_location[1]] == 1:\n",
    "                self._box_location = self._prev_box_location\n",
    "                return False, False\n",
    "            else:\n",
    "                self._agent_location = self._agent_location + self._action_to_direction[push_dir]\n",
    "                return True, True\n",
    "        \n",
    "        return False, False\n",
    "            \n",
    "    def _move(self,action):\n",
    "            self._prev_agent_location = self._agent_location\n",
    "            self._prev_box_location = self._box_location\n",
    "            self._agent_location = self._agent_location + self._action_to_direction[action]\n",
    "#             print(self.GRID[self._agent_location],self._agent_location,self.GRID)\n",
    "            if self.GRID[self._agent_location[0],self._agent_location[1]] == 1:\n",
    "                self._agent_location = self._prev_agent_location\n",
    "                return False\n",
    "            elif np.array_equal(self._agent_location, self._box_location):\n",
    "                self._agent_location = self._prev_agent_location\n",
    "                return False\n",
    "            return True\n",
    "    def is_over(self):\n",
    "        if np.array_equal(self._box_location, self._target_location):\n",
    "            done = True\n",
    "            reward = 0\n",
    "        elif sum([True if self.GRID[(self._box_location + val)[0],(self._box_location + val)[1]] == 1 else False for val in self._action_to_direction.values()])>1 :\n",
    "            done = True\n",
    "            reward = -1\n",
    "        else: \n",
    "            done = False\n",
    "            reward = -1\n",
    "        return done , reward\n",
    "            \n",
    "                \n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "250a666a-789e-47ae-af3d-b2761f905766",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = WarehouseAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "30449565-9d31-4990-b7b6-3a2721c567e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._get_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8a292d0-7d01-478d-971c-57f246d5ab75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', '1', '1', '1', '1', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', '0', 'A', '1', '1', '1'],\n",
       "       ['1', 'T', '0', '0', '0', '1'],\n",
       "       ['1', '0', '0', 'B', '0', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', '1', '1', '1', '1', '1']], dtype='<U1')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3556f52-59fa-4a8e-a29a-d641bef5a930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', '1', '1', '1', '1', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', 'T', 'A', '0', '0', '1'],\n",
       "       ['1', '0', '0', 'B', '0', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', '1', '1', '1', '1', '1']], dtype='<U1')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cb5f34d3-3870-45f8-95b3-7eaa6fe89502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', '1', '1', '1', '1', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', 'T', '0', 'A', '0', '1'],\n",
       "       ['1', '0', '0', 'B', '0', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', '1', '1', '1', '1', '1']], dtype='<U1')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(3)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ebc14551-4725-4eda-a740-c6352b38070a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', '1', '1', '1', '1', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', 'T', '0', '0', 'A', '1'],\n",
       "       ['1', '0', '0', 'B', '0', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', '1', '1', '1', '1', '1']], dtype='<U1')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(3)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b43d42e6-00b8-4979-9d56-3c75dc113bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', '1', '1', '1', '1', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', 'T', '0', '0', '0', '1'],\n",
       "       ['1', '0', '0', 'B', 'A', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', '1', '1', '1', '1', '1']], dtype='<U1')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ffb7cd7d-e63a-4797-b08a-3d14ff6f450d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', '1', '1', '1', '1', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', 'T', '0', '0', '0', '1'],\n",
       "       ['1', '0', 'B', 'A', '0', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', '1', '1', '1', '1', '1']], dtype='<U1')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(4)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "948198d2-1ccc-45cd-9d02-ae5583db95a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', '1', '1', '1', '1', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', 'T', '0', '0', '0', '1'],\n",
       "       ['1', 'B', 'A', '0', '0', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', '1', '1', '1', '1', '1']], dtype='<U1')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(4)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4840fad6-3e77-4af4-94fa-9dab258b3367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', '1', '1', '1', '1', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', 'T', '0', '0', '0', '1'],\n",
       "       ['1', 'B', '0', '0', '0', '1'],\n",
       "       ['1', '0', 'A', '1', '1', '1'],\n",
       "       ['1', '1', '1', '1', '1', '1']], dtype='<U1')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4b5a59ab-4a79-4754-85d1-1fd7c3ad5e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', '1', '1', '1', '1', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', 'T', '0', '0', '0', '1'],\n",
       "       ['1', 'B', '0', '0', '0', '1'],\n",
       "       ['1', 'A', '0', '1', '1', '1'],\n",
       "       ['1', '1', '1', '1', '1', '1']], dtype='<U1')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2bcb43a6-5347-44f0-ad05-13cf04d61480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', '1', '1', '1', '1', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', 'T', '0', '0', '0', '1'],\n",
       "       ['1', 'A', '0', '0', '0', '1'],\n",
       "       ['1', '0', '0', '1', '1', '1'],\n",
       "       ['1', '1', '1', '1', '1', '1']], dtype='<U1')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(4)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7fa7de7f-7884-431f-bb26-45056f28764a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent': array([4, 1]), 'box': array([3, 1]), 'target': array([3, 1])}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._get_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1b9e9e08-e1af-45e7-9ca7-42149606dcf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env._get_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cbdce37d-211c-4526-865d-349112d7f046",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1000\n",
      "[['1' '1' '1' '1' '1' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' 'T' 'A' '0' '0' '1']\n",
      " ['1' '0' '0' '0' 'B' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' '1' '1' '1' '1' '1']]\n"
     ]
    }
   ],
   "source": [
    "ep = 1\n",
    "for eps in range(ep):\n",
    "    rw = 0\n",
    "    \n",
    "    for i in range(1000):\n",
    "        act = np.random.randint(0,5)\n",
    "#         print(act)\n",
    "        observation, reward, done, info = env.step(act)\n",
    "        rw = rw + reward\n",
    "#         print(act)\n",
    "    print(rw)\n",
    "    print(env.render())\n",
    "    env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ba0b675e-2125-49e2-8e1d-12508b7c5247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e0355604-2bce-49e5-98c7-6ba087557199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_epsilon_greedy(env, num_episodes, discount_factor=1.0, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Monte Carlo Control using Epsilon-Greedy policies.\n",
    "    Finds an optimal epsilon-greedy policy.\n",
    " \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    " \n",
    "    Returns:\n",
    "        A tuple (Q, policy).\n",
    "        Q is a dictionary mapping state to action values.\n",
    "        policy is a function that takes an observation as an argument and returns\n",
    "        action probabilities\n",
    "    \"\"\"\n",
    " \n",
    "    # Keeps track of sum and count of returns for each state\n",
    "    # to calculate an average. We could use an array to save all\n",
    "    # returns (like in the book) but that's memory inefficient.\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    " \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state to (action to action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    # A nested dictionary that maps state to (action to number of times state-action pair was encountered).\n",
    "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    iterations = 0\n",
    "    # policy improvement: this function holds a reference to the Q_values\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    while iterations < num_episodes:\n",
    "        done = False\n",
    "        episode = []\n",
    "        visited_states = {}\n",
    "        s = env.reset()\n",
    "        while not done:\n",
    "            # choose an action based on a probability dist generated by policy(), epsilon/ |A(s)| chance of random action\n",
    "            action = np.random.choice(range(env.action_space.n), p=policy(s))\n",
    "            new_s, r, done, _ = env.step(action)\n",
    "            episode.append((s, action, r))\n",
    "        for state,action,reward in episode[::-1]:\n",
    "            # first-visit monte carlo update\n",
    "            if state not in visited_states:\n",
    "                N[state][action] += 1\n",
    "                # incremental update of Q value is more memory efficient than simply keeping a record of all rewards\n",
    "                # and averaging after every new reward\n",
    "                Q[state][action] += discount_factor * ( 1./ N[state][action] ) * (reward - Q[state][action])\n",
    "                visited_states.add(state)\n",
    " \n",
    "        iterations += 1\n",
    " \n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "39f44cd9-4ca9-40de-8753-433436767c75",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_epsilon_greedy_policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18068\\2327872299.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWarehouseAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmc_control_epsilon_greedy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18068\\333523075.py\u001b[0m in \u001b[0;36mmc_control_epsilon_greedy\u001b[1;34m(env, num_episodes, discount_factor, epsilon)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0miterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# policy improvement: this function holds a reference to the Q_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_epsilon_greedy_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0miterations\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'make_epsilon_greedy_policy' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "env = WarehouseAgent()\n",
    "mc_control_epsilon_greedy(env,100,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc2e909-88ab-4835-a5b4-5e373c389c25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
