{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40fa88ea-c53c-46cd-befd-23dbfa5fa031",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a6db4f7-22ce-4767-89b8-f09655f57664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "import gym\n",
    "import pygame\n",
    "from gym.spaces import Discrete, Box, Dict\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "408b6b42-2ddc-46c9-a895-2f8b8d4ae36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarehouseAgent:\n",
    "    def __init__(self):\n",
    "        self.GRID_DIM = [7, 6]\n",
    "\n",
    "        self.agent_position = [1, 2]\n",
    "\n",
    "        self.box_location = [4, 3]\n",
    "        self.goal_location = [3, 1]\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([-1, 0]),\n",
    "            1: np.array([1, 0]),\n",
    "            2: np.array([0, -1]),\n",
    "            3: np.array([0, 1]),\n",
    "        }\n",
    "        self._ACTIONLOOKUP = {\n",
    "            0: \"move up\",\n",
    "            1: \"move down\",\n",
    "            2: \"move left\",\n",
    "            3: \"move right\",\n",
    "            4: \"push\",\n",
    "        }\n",
    "        self.GRID_DIM = np.asarray(self.GRID_DIM)\n",
    "        self.GRID = np.zeros(\n",
    "            self.GRID_DIM\n",
    "        )  # The Boundaries are the walls, so playing space is only [:-2,:-2]\n",
    "        self.GRID[:, [0, -1]] = 1\n",
    "        self.GRID[[0, -1], :] = 1\n",
    "        self.GRID[[1, 2, 5], 3:5] = 1\n",
    "        self.walls = 1\n",
    "        self.action_space = Discrete(len(self._ACTIONLOOKUP.keys()))\n",
    "        self.state_space = Discrete(self.GRID_DIM[0] * self.GRID_DIM[1])\n",
    "        self.observation_space = Dict(\n",
    "            {\n",
    "                \"agent\": Box(\n",
    "                    np.array([0, 0]),\n",
    "                    np.array([self.GRID_DIM[0] - 1, self.GRID_DIM[1] - 1]),\n",
    "                    shape=(2,),\n",
    "                    dtype=int,\n",
    "                ),\n",
    "                \"box\": Box(\n",
    "                    np.array([0, 0]),\n",
    "                    np.array([self.GRID_DIM[0] - 1, self.GRID_DIM[1] - 1]),\n",
    "                    shape=(2,),\n",
    "                    dtype=int,\n",
    "                ),\n",
    "                \"target\": Box(\n",
    "                    np.array([0, 0]),\n",
    "                    np.array([self.GRID_DIM[0] - 1, self.GRID_DIM[1] - 1]),\n",
    "                    shape=(2,),\n",
    "                    dtype=int,\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        self._agent_location = np.array(self.agent_position)\n",
    "        self._box_location = np.array(self.box_location)\n",
    "        self._target_location = np.array(self.goal_location)\n",
    "\n",
    "    #         print(self.GRID)\n",
    "\n",
    "    def step(self, action):\n",
    "        self._prev_agent_location = None\n",
    "        self._prev_box_location = None\n",
    "        moved_box = False\n",
    "\n",
    "        if action < 4:\n",
    "            moved_player = self._move(action)\n",
    "        else:\n",
    "            moved_player, moved_box = self._push(action)\n",
    "\n",
    "        done, reward = self.is_over()\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        rend = self.GRID.copy().astype(dtype=\"U1\")\n",
    "        rend[self._agent_location[0], self._agent_location[1]] = \"A\"\n",
    "        rend[self._box_location[0], self._box_location[1]] = \"B\"\n",
    "        rend[self._target_location[0], self._target_location[1]] = \"T\"\n",
    "        if np.array_equal(self._target_location, self._box_location):\n",
    "            rend[self._target_location[0], self._target_location[1]] = \"D\"\n",
    "        return print(rend)\n",
    "\n",
    "    def reset(self, seed=None, return_info=False, options=None):\n",
    "        self._agent_location = np.array(self.agent_position)\n",
    "        self._box_location = np.array(self.box_location)\n",
    "        self._target_location = np.array(self.goal_location)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        return (observation, info) if return_info else observation\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "            \"agent\": self._agent_location,\n",
    "            \"box\": self._box_location,\n",
    "            \"target\": self._target_location,\n",
    "        }\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self._box_location - self._target_location, ord=1\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def _state_in_seq(self):\n",
    "        m, n = self._agent_location\n",
    "        seq = m * self.GRID.shape[1] + n\n",
    "        return seq\n",
    "\n",
    "    def _push(self, action):\n",
    "        loc = self._box_location - self._agent_location\n",
    "        #         print(f'loc{loc}, box :{self._box_location}, agent:{self._agent_location}')\n",
    "        push_dir = None\n",
    "        for idx, val in enumerate(self._action_to_direction.values()):\n",
    "            if np.array_equal(loc, val):\n",
    "                valid = True\n",
    "                push_dir = idx\n",
    "                break\n",
    "            else:\n",
    "                valid = False\n",
    "\n",
    "        if valid:\n",
    "            self._prev_agent_location = self._agent_location\n",
    "            self._prev_box_location = self._box_location\n",
    "            self._box_location = (\n",
    "                self._box_location + self._action_to_direction[push_dir]\n",
    "            )\n",
    "            if self.GRID[self._box_location[0], self._box_location[1]] == 1:\n",
    "                self._box_location = self._prev_box_location\n",
    "                return False, False\n",
    "            else:\n",
    "                self._agent_location = (\n",
    "                    self._agent_location + self._action_to_direction[push_dir]\n",
    "                )\n",
    "                return True, True\n",
    "\n",
    "        return False, False\n",
    "\n",
    "    def _move(self, action):\n",
    "        self._prev_agent_location = self._agent_location\n",
    "        self._prev_box_location = self._box_location\n",
    "        self._agent_location = self._agent_location + self._action_to_direction[action]\n",
    "        #             print(self.GRID[self._agent_location],self._agent_location,self.GRID)\n",
    "        if self.GRID[self._agent_location[0], self._agent_location[1]] == 1:\n",
    "            self._agent_location = self._prev_agent_location\n",
    "            return False\n",
    "        elif np.array_equal(self._agent_location, self._box_location):\n",
    "            self._agent_location = self._prev_agent_location\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def is_over(self):\n",
    "        if np.array_equal(\n",
    "            self._box_location, self._target_location\n",
    "        ):  # checking if the box is at the target already\n",
    "            done = True\n",
    "            reward = 0\n",
    "        elif (\n",
    "            sum(\n",
    "                a := np.array(\n",
    "                    [\n",
    "                        True\n",
    "                        if self.GRID[\n",
    "                            (self._box_location + val)[0], (self._box_location + val)[1]\n",
    "                        ]\n",
    "                        == 1\n",
    "                        else False\n",
    "                        for val in self._action_to_direction.values()\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            >= 1\n",
    "        ):\n",
    "            # basically checking if there are atleast 1 wall adjacent to box\n",
    "            if sum(a) > 1:\n",
    "                done = True\n",
    "                reward = -1\n",
    "            elif sum(a) == 1:\n",
    "                if ~(self._box_location - self._target_location).all():\n",
    "                    done = False\n",
    "                    reward = -1\n",
    "                    return done, reward\n",
    "                else:\n",
    "                    #                 print(a)\n",
    "                    direc = np.where(a == True)\n",
    "                    #                 print(direc)\n",
    "                    direc = direc[0][0]\n",
    "                    left = self._box_location + self._action_to_direction[direc]\n",
    "                    right = left.copy()\n",
    "                    if direc in [0, 1]:\n",
    "                        count = 0\n",
    "                        while (self.GRID[left[0], left[1]] != 0) and (\n",
    "                            self.GRID[right[0], right[1]] != 0\n",
    "                        ):\n",
    "\n",
    "                            left = np.clip(\n",
    "                                left + self._action_to_direction[2],\n",
    "                                [0, 0],\n",
    "                                [self.GRID_DIM[0] - 1, self.GRID_DIM[1] - 1],\n",
    "                            )\n",
    "                            right = np.clip(\n",
    "                                right + self._action_to_direction[3],\n",
    "                                [0, 0],\n",
    "                                [self.GRID_DIM[0] - 1, self.GRID_DIM[1] - 1],\n",
    "                            )\n",
    "                            count += 1\n",
    "                            if count >= self.GRID_DIM[1]:\n",
    "                                done = True\n",
    "                                reward = -1\n",
    "                                return done, reward\n",
    "                                break\n",
    "                    #                         right = right + self._action_to_direction[3]\n",
    "\n",
    "                    else:\n",
    "                        count = 0\n",
    "                        while (self.GRID[left[0], left[1]] != 0) and (\n",
    "                            self.GRID[right[0], right[1]] != 0\n",
    "                        ):\n",
    "                            left = np.clip(\n",
    "                                left + self._action_to_direction[1],\n",
    "                                [0, 0],\n",
    "                                [self.GRID_DIM[0] - 1, self.GRID_DIM[1] - 1],\n",
    "                            )\n",
    "                            right = np.clip(\n",
    "                                right + self._action_to_direction[0],\n",
    "                                [0, 0],\n",
    "                                [self.GRID_DIM[0] - 1, self.GRID_DIM[1] - 1],\n",
    "                            )\n",
    "                            count += 1\n",
    "                            if count >= self.GRID_DIM[0]:\n",
    "                                done = True\n",
    "                                reward = -1\n",
    "                                return done, reward\n",
    "                                break\n",
    "\n",
    "                    done = False\n",
    "                    reward = -1\n",
    "                    return done, reward\n",
    "        #         np.where([True if self.GRID[(self._box_location + val)[0], (self._box_location + val)[1] ] == 1 else False for val in self._action_to_direction.values() ] == True)[0][0]: # gotta check if the box is not adjacent to 2 walls but still is terminating state like the boundary walls\n",
    "        else:\n",
    "            done = False\n",
    "            reward = -1\n",
    "        return done, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "160ec851-9b68-4ada-9dca-d66e4ea05b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1' '1' '1' '1' '1' '1']\n",
      " ['1' '0' 'A' '1' '1' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' 'T' '0' '0' '0' '1']\n",
      " ['1' '0' '0' 'B' '0' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' '1' '1' '1' '1' '1']]\n"
     ]
    }
   ],
   "source": [
    "env = WarehouseAgent()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1daf01f5-2c89-4e98-a586-fbe727082dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'agent': array([2, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "[['1' '1' '1' '1' '1' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' '0' 'A' '1' '1' '1']\n",
      " ['1' 'T' '0' '0' '0' '1']\n",
      " ['1' '0' '0' 'B' '0' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' '1' '1' '1' '1' '1']]\n",
      "None\n",
      "({'agent': array([3, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "[['1' '1' '1' '1' '1' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' 'T' 'A' '0' '0' '1']\n",
      " ['1' '0' '0' 'B' '0' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' '1' '1' '1' '1' '1']]\n",
      "None\n",
      "({'agent': array([3, 3]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "[['1' '1' '1' '1' '1' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' 'T' '0' 'A' '0' '1']\n",
      " ['1' '0' '0' 'B' '0' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' '1' '1' '1' '1' '1']]\n",
      "None\n",
      "({'agent': array([3, 4]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "[['1' '1' '1' '1' '1' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' 'T' '0' '0' 'A' '1']\n",
      " ['1' '0' '0' 'B' '0' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' '1' '1' '1' '1' '1']]\n",
      "None\n",
      "({'agent': array([4, 4]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "[['1' '1' '1' '1' '1' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' 'T' '0' '0' '0' '1']\n",
      " ['1' '0' '0' 'B' 'A' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' '1' '1' '1' '1' '1']]\n",
      "None\n",
      "({'agent': array([4, 3]), 'box': array([4, 2]), 'target': array([3, 1])}, -1, False, {'distance': 2.0})\n",
      "[['1' '1' '1' '1' '1' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' 'T' '0' '0' '0' '1']\n",
      " ['1' '0' 'B' 'A' '0' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' '1' '1' '1' '1' '1']]\n",
      "None\n",
      "({'agent': array([3, 3]), 'box': array([4, 2]), 'target': array([3, 1])}, -1, False, {'distance': 2.0})\n",
      "[['1' '1' '1' '1' '1' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' 'T' '0' 'A' '0' '1']\n",
      " ['1' '0' 'B' '0' '0' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' '1' '1' '1' '1' '1']]\n",
      "None\n",
      "({'agent': array([3, 2]), 'box': array([4, 2]), 'target': array([3, 1])}, -1, False, {'distance': 2.0})\n",
      "[['1' '1' '1' '1' '1' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' 'T' 'A' '0' '0' '1']\n",
      " ['1' '0' 'B' '0' '0' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' '1' '1' '1' '1' '1']]\n",
      "None\n",
      "({'agent': array([4, 2]), 'box': array([5, 2]), 'target': array([3, 1])}, -1, True, {'distance': 3.0})\n",
      "[['1' '1' '1' '1' '1' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' '0' '0' '1' '1' '1']\n",
      " ['1' 'T' '0' '0' '0' '1']\n",
      " ['1' '0' 'A' '0' '0' '1']\n",
      " ['1' '0' 'B' '1' '1' '1']\n",
      " ['1' '1' '1' '1' '1' '1']]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "act = [1, 1, 3, 3, 1, 4, 0, 2, 4]\n",
    "for ac in act:\n",
    "    print(env.step(ac))\n",
    "    print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f60a9ce-83eb-4d80-b054-ef490b552ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~(np.array([4, 1]) - np.array([1, 3])).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600c099a-5aea-49b4-9ea5-2771d36d51eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ffb64a-6057-4569-ab70-5cc395b7517e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa4548c-ad99-4558-896f-71a026c03998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5fe385-1f33-425e-ae73-5efeb6accf63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbcb6e6-3667-439f-a8b2-a462ff0e1769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a528a9-9302-4110-ad3c-e2002d01fb89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30d45a18-d50e-466a-90aa-e6926697bd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KHADGA JYOTH ALLI\\anaconda3\\lib\\site-packages\\seaborn\\rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(mpl.__version__) >= \"3.0\":\n",
      "C:\\Users\\KHADGA JYOTH ALLI\\anaconda3\\lib\\site-packages\\setuptools\\_distutils\\version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import operator\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import random\n",
    "import itertools\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "tqdm.monitor_interval = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34c5d1ec-7915-4351-8e9a-719153950d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_policy(env):\n",
    "    policy = {}\n",
    "    for key in range(0, env.state_space.n):\n",
    "        current_end = 0\n",
    "        p = {}\n",
    "        for action in range(0, env.action_space.n):\n",
    "            p[action] = 1 / env.action_space.n\n",
    "        policy[key] = p\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf89f219-3471-4921-ae7e-a4e195091a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_action_dictionary(env, policy):\n",
    "    Q = {}\n",
    "    for key in policy.keys():\n",
    "        Q[key] = {a: 0.0 for a in range(0, env.action_space.n)}\n",
    "    return Q\n",
    "cre\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab6ce6c4-3389-4676-a5c4-c0ddd9079b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(env, policy, display=True, returns=True):\n",
    "    env.reset()\n",
    "    episode = []\n",
    "    returns = []\n",
    "    finished = False\n",
    "    while not finished:\n",
    "        s = env._state_in_seq()\n",
    "        #         print(s)  # env._get_obs()[\"agent\"][0] * env._get_obs()[\"agent\"][1]\n",
    "        #         if display:\n",
    "        #             clear_output(True)\n",
    "        #             print(env.render())\n",
    "        #             sleep(1)\n",
    "\n",
    "        timestep = []\n",
    "        timestep.append(s)\n",
    "        n = random.uniform(0, sum(policy[s].values()))\n",
    "        top_range = 0\n",
    "        for prob in policy[s].items():\n",
    "            top_range += prob[1]\n",
    "            if n < top_range:\n",
    "                action = prob[0]\n",
    "                break\n",
    "        #         print(action)\n",
    "        state, reward, finished, info = env.step(action)\n",
    "        timestep.append(action)\n",
    "        timestep.append(reward)\n",
    "\n",
    "        episode.append(timestep)\n",
    "        returns.append(reward)\n",
    "\n",
    "    #         print(state, reward)\n",
    "\n",
    "    if display:\n",
    "        clear_output(True)\n",
    "        print(env.render())\n",
    "        sleep(1)\n",
    "    if returns:\n",
    "        return episode, returns\n",
    "    return episode, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a56f422-3e3e-494e-9bf6-253e59276a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(policy, env, r=10):\n",
    "    wins = 0\n",
    "    r = 100\n",
    "    for i in range(r):\n",
    "        w, _ = run_game(env, policy, display=True)\n",
    "        w = w[-1][-1]\n",
    "        if w == 0:\n",
    "            wins += 1\n",
    "    return wins / r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52cd9f18-30dd-4578-9c63-a34462f99d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_e_soft(env, episodes=100, policy=None, epsilon=0.01, plot_graph=True):\n",
    "    if not policy:\n",
    "        policy = create_random_policy(\n",
    "            env\n",
    "        )  # Create an empty dictionary to store state action values\n",
    "    Q = create_state_action_dictionary(\n",
    "        env, policy\n",
    "    )  # Empty dictionary for storing rewards for each state-action pair\n",
    "    returns = {}  # 3.\n",
    "    rx = []\n",
    "    for _ in range(episodes):  # Looping through episodes\n",
    "        G = 0  # Store cumulative reward in G (initialized at 0)\n",
    "        episode, rew = run_game(\n",
    "            env=env, policy=policy, display=False\n",
    "        )  # Store state, action and value respectively\n",
    "        rx.append(np.sum(rew))\n",
    "        # for loop through reversed indices of episode array.\n",
    "        # The logic behind it being reversed is that the eventual reward would be at the end.\n",
    "        # So we have to go back from the last timestep to the first one propagating result from the future.\n",
    "\n",
    "        for i in reversed(range(0, len(episode))):\n",
    "            s_t, a_t, r_t = episode[i]\n",
    "            state_action = (s_t, a_t)\n",
    "            G += r_t  # Increment total reward by reward on current timestep\n",
    "\n",
    "            if not state_action in [(x[0], x[1]) for x in episode[0:i]]:  #\n",
    "                if returns.get(state_action):\n",
    "                    returns[state_action].append(G)\n",
    "                else:\n",
    "                    returns[state_action] = [G]\n",
    "\n",
    "                Q[s_t][a_t] = sum(returns[state_action]) / len(\n",
    "                    returns[state_action]\n",
    "                )  # Average reward across episodes\n",
    "\n",
    "                Q_list = list(\n",
    "                    map(lambda x: x[1], Q[s_t].items())\n",
    "                )  # Finding the action with maximum value\n",
    "                indices = [i for i, x in enumerate(Q_list) if x == max(Q_list)]\n",
    "                max_Q = random.choice(indices)\n",
    "\n",
    "                A_star = max_Q  # 14.\n",
    "\n",
    "                for a in policy[s_t].items():\n",
    "                    #                     print(policy[s_t])  # Update action probability for s_t in policy\n",
    "                    if a[0] == A_star:\n",
    "\n",
    "                        policy[s_t][a[0]] = (\n",
    "                            1\n",
    "                            - epsilon\n",
    "                            + (\n",
    "                                epsilon\n",
    "                                / np.abs(np.sum(np.array(list(policy[s_t].values()))))\n",
    "                            )\n",
    "                        )\n",
    "                    else:\n",
    "                        policy[s_t][a[0]] = epsilon / np.abs(\n",
    "                            np.sum(np.array(list(policy[s_t].values())))\n",
    "                        )  # abs(sum(policy[s_t].values()))\n",
    "    print(\"N\")\n",
    "    if plot_graph:\n",
    "        print(\"Y\")\n",
    "        cumulative_average = np.cumsum(np.array(rx)) / (np.arange(episodes) + 1)\n",
    "        plt.plot(\n",
    "            cumulative_average, label=r\"Gradient Bandit ($\\alpha$ = \" + f\"{epsilon})\"\n",
    "        )\n",
    "        plt.title(f\"Average rewards over {episodes} timesteps\", fontsize=14)\n",
    "        plt.xlabel(\"Timesteps\", fontsize=14)\n",
    "        plt.ylabel(\"Average Rewards\", fontsize=14)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d311f89-8b69-42f3-baea-49ef177ab127",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = WarehouseAgent()\n",
    "env.reset()\n",
    "policy = monte_carlo_e_soft(env, episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d06e8e-7f42-4935-b83b-7fef2bcf3e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a257cb69-dc64-47d7-8a97-37001afc1056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_policy(policy):\n",
    "    plot = np.zeros(len(policy.keys()), dtype=\"U2\")\n",
    "    sign = {0: \"^\", 1: \"v\", 2: \"<\", 3: \">\", 4: \"P\"}\n",
    "    for keys, values in zip(policy.keys(), policy.values()):\n",
    "        lst = []\n",
    "        #         print(values)\n",
    "        for val in values.values():\n",
    "            lst.append(val)\n",
    "        #         print(lst)\n",
    "        direc = np.argmax(np.asarray(lst))\n",
    "        plot[keys] = sign[direc]\n",
    "    plot = plot.reshape([7, 6])\n",
    "    return plot\n",
    "\n",
    "\n",
    "env.render()\n",
    "plot_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3927dea2-298e-49cc-bcd9-dfeba44fefe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfad54d-7281-4ac0-85f8-9f120d5b64dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_policy(policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31a2bd3-5967-4056-b823-9682c6fa5db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6a89d0-e6af-45ba-a3bf-c84e43d265c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = [0] * 11\n",
    "if (a := len(z)) > 10:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84334d-78dc-4e47-80d2-103ec6b5d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (n := len(z)) > 10:\n",
    "    print(f\"List is too long ({n} elements, expected <= 10)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e07def-1264-44c1-90d0-1b942c91314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = np.array([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaba6ced-d611-4546-a427-619c62e9c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba0306-002f-4194-ba48-e038034d4e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env._get_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd29e84-ef6d-4e5b-81bf-f0a633db84aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env._state_in_seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745ffaef-03ae-416b-8fd1-bedeacd29a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_game(policy=policy, env=env, display=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5559b3f8-f0ca-4791-9c1d-f5c86df9cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb7e120-9d3e-4943-bb17-c1051f83788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if sum(a := [1, 1, 0, 0]) > 1:\n",
    "#     print(True)\n",
    "_ = np.array([True, True, False])\n",
    "np.where(_ == True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633554c4-d0d8-4d47-b2f2-72f3df79d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list({0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6a1a91-96c4-432a-b14f-ab1e19e86355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
