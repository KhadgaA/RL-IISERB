{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb5733b-1d93-4204-9e3d-1dea5088ec5f",
   "metadata": {},
   "source": [
    "Should be final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d883a8f2-db74-4313-9fc6-78dee1c590b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "import gym\n",
    "# import pygame\n",
    "from gym.spaces import Discrete, Box, Dict\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e373aa9-e82b-43c0-b870-7d9b86bdf2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarehouseAgent:\n",
    "    def __init__(self):\n",
    "        self.GRID_DIM = [7, 6]\n",
    "\n",
    "        self.agent_position = [1, 2]\n",
    "\n",
    "        self.box_location = [4, 3]\n",
    "        self.goal_location = [3, 1]\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([-1, 0]),\n",
    "            1: np.array([1, 0]),\n",
    "            2: np.array([0, -1]),\n",
    "            3: np.array([0, 1]),\n",
    "        }\n",
    "        self._ACTIONLOOKUP = {\n",
    "            0: \"move up\",\n",
    "            1: \"move down\",\n",
    "            2: \"move left\",\n",
    "            3: \"move right\",\n",
    "            4: \"push\",\n",
    "        }\n",
    "        self.GRID_DIM = np.asarray(self.GRID_DIM)\n",
    "        self.GRID = np.zeros(\n",
    "            self.GRID_DIM\n",
    "        )  # The Boundaries are the walls, so playing space is only [:-2,:-2]\n",
    "        self.GRID[:, [0, -1]] = 1\n",
    "        self.GRID[[0, -1], :] = 1\n",
    "        self.GRID[[1, 2, 5], 3:5] = 1\n",
    "        self.walls = 1\n",
    "        self.action_space = Discrete(len(self._ACTIONLOOKUP.keys()))\n",
    "        self.state_space = Discrete(self.GRID_DIM[0] * self.GRID_DIM[1])\n",
    "        self.observation_space = Dict(\n",
    "            {\n",
    "                \"agent\": Box(\n",
    "                    np.array([0, 0]),\n",
    "                    np.array([self.GRID_DIM[0] - 1, self.GRID_DIM[1] - 1]),\n",
    "                    shape=(2,),\n",
    "                    dtype=int,\n",
    "                ),\n",
    "                \"box\": Box(\n",
    "                    np.array([0, 0]),\n",
    "                    np.array([self.GRID_DIM[0] - 1, self.GRID_DIM[1] - 1]),\n",
    "                    shape=(2,),\n",
    "                    dtype=int,\n",
    "                ),\n",
    "                \"target\": Box(\n",
    "                    np.array([0, 0]),\n",
    "                    np.array([self.GRID_DIM[0] - 1, self.GRID_DIM[1] - 1]),\n",
    "                    shape=(2,),\n",
    "                    dtype=int,\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        self._agent_location = np.array(self.agent_position)\n",
    "        self._box_location = np.array(self.box_location)\n",
    "        self._target_location = np.array(self.goal_location)\n",
    "\n",
    "    #         print(self.GRID)\n",
    "\n",
    "    def step(self, action):\n",
    "        self._prev_agent_location = None\n",
    "        self._prev_box_location = None\n",
    "        moved_box = False\n",
    "\n",
    "        if action < 4:\n",
    "            moved_player = self._move(action)\n",
    "        else:\n",
    "            moved_player, moved_box = self._push(action)\n",
    "\n",
    "        done, reward = self.is_over()\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        rend = self.GRID.copy().astype(dtype=\"U1\")\n",
    "        rend[self._agent_location[0], self._agent_location[1]] = \"A\"\n",
    "        rend[self._box_location[0], self._box_location[1]] = \"B\"\n",
    "        rend[self._target_location[0], self._target_location[1]] = \"T\"\n",
    "        if np.array_equal(self._target_location, self._box_location):\n",
    "            rend[self._target_location[0], self._target_location[1]] = \"D\"\n",
    "        return print(rend)\n",
    "\n",
    "    def reset(self, seed=None, return_info=False, options=None):\n",
    "        self._agent_location = np.array(self.agent_position)\n",
    "        self._box_location = np.array(self.box_location)\n",
    "        self._target_location = np.array(self.goal_location)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        return (observation, info) if return_info else observation\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "            \"agent\": self._agent_location,\n",
    "            \"box\": self._box_location,\n",
    "            \"target\": self._target_location,\n",
    "        }\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self._box_location - self._target_location, ord=1\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def _state_in_seq(self):\n",
    "        m, n = self._agent_location\n",
    "        seq = m * self.GRID.shape[1] + n\n",
    "        return seq\n",
    "\n",
    "    def _push(self, action):\n",
    "        loc = self._box_location - self._agent_location\n",
    "        #         print(f'loc{loc}, box :{self._box_location}, agent:{self._agent_location}')\n",
    "        push_dir = None\n",
    "        for idx, val in enumerate(self._action_to_direction.values()):\n",
    "            if np.array_equal(loc, val):\n",
    "                valid = True\n",
    "                push_dir = idx\n",
    "                break\n",
    "            else:\n",
    "                valid = False\n",
    "\n",
    "        if valid:\n",
    "            self._prev_agent_location = self._agent_location\n",
    "            self._prev_box_location = self._box_location\n",
    "            self._box_location = (\n",
    "                self._box_location + self._action_to_direction[push_dir]\n",
    "            )\n",
    "            if self.GRID[self._box_location[0], self._box_location[1]] == 1:\n",
    "                self._box_location = self._prev_box_location\n",
    "                return False, False\n",
    "            else:\n",
    "                self._agent_location = (\n",
    "                    self._agent_location + self._action_to_direction[push_dir]\n",
    "                )\n",
    "                return True, True\n",
    "\n",
    "        return False, False\n",
    "\n",
    "    def _move(self, action):\n",
    "        self._prev_agent_location = self._agent_location\n",
    "        self._prev_box_location = self._box_location\n",
    "        self._agent_location = self._agent_location + self._action_to_direction[action]\n",
    "        #             print(self.GRID[self._agent_location],self._agent_location,self.GRID)\n",
    "        if self.GRID[self._agent_location[0], self._agent_location[1]] == 1:\n",
    "            self._agent_location = self._prev_agent_location\n",
    "            return False\n",
    "        elif np.array_equal(self._agent_location, self._box_location):\n",
    "            self._agent_location = self._prev_agent_location\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def is_over(self):\n",
    "        if np.array_equal(\n",
    "            self._box_location, self._target_location\n",
    "        ):  # checking if the box is at the target already\n",
    "            done = True\n",
    "            reward = 0\n",
    "        elif (\n",
    "            sum(\n",
    "                a := np.array(\n",
    "                    [\n",
    "                        True\n",
    "                        if self.GRID[\n",
    "                            (self._box_location + val)[0], (self._box_location + val)[1]\n",
    "                        ]\n",
    "                        == 1\n",
    "                        else False\n",
    "                        for val in self._action_to_direction.values()\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            >= 1\n",
    "        ):\n",
    "            # basically checking if there are atleast 1 wall adjacent to box\n",
    "            if sum(a) > 1:\n",
    "                done = True\n",
    "                reward = -1\n",
    "            elif sum(a) == 1:\n",
    "                if ~(self._box_location - self._target_location).all():\n",
    "                    done = False\n",
    "                    reward = -1\n",
    "                    return done, reward\n",
    "                else:\n",
    "                    #                 print(a)\n",
    "                    direc = np.where(a == True)\n",
    "                    #                 print(direc)\n",
    "                    direc = direc[0][0]\n",
    "                    left = self._box_location + self._action_to_direction[direc]\n",
    "                    right = left.copy()\n",
    "                    if direc in [0, 1]:\n",
    "                        count = 0\n",
    "                        while (self.GRID[left[0], left[1]] != 0) and (\n",
    "                            self.GRID[right[0], right[1]] != 0\n",
    "                        ):\n",
    "\n",
    "                            left = np.clip(\n",
    "                                left + self._action_to_direction[2],\n",
    "                                [0, 0],\n",
    "                                [self.GRID_DIM[0] - 1, self.GRID_DIM[1] - 1],\n",
    "                            )\n",
    "                            right = np.clip(\n",
    "                                right + self._action_to_direction[3],\n",
    "                                [0, 0],\n",
    "                                [self.GRID_DIM[0] - 1, self.GRID_DIM[1] - 1],\n",
    "                            )\n",
    "                            count += 1\n",
    "                            if count >= self.GRID_DIM[1]:\n",
    "                                done = True\n",
    "                                reward = -1\n",
    "                                return done, reward\n",
    "                                break\n",
    "                    #                         right = right + self._action_to_direction[3]\n",
    "\n",
    "                    else:\n",
    "                        count = 0\n",
    "                        while (self.GRID[left[0], left[1]] != 0) and (\n",
    "                            self.GRID[right[0], right[1]] != 0\n",
    "                        ):\n",
    "                            left = np.clip(\n",
    "                                left + self._action_to_direction[1],\n",
    "                                [0, 0],\n",
    "                                [self.GRID_DIM[0] - 1, self.GRID_DIM[1] - 1],\n",
    "                            )\n",
    "                            right = np.clip(\n",
    "                                right + self._action_to_direction[0],\n",
    "                                [0, 0],\n",
    "                                [self.GRID_DIM[0] - 1, self.GRID_DIM[1] - 1],\n",
    "                            )\n",
    "                            count += 1\n",
    "                            if count >= self.GRID_DIM[0]:\n",
    "                                done = True\n",
    "                                reward = -1\n",
    "                                return done, reward\n",
    "                                break\n",
    "\n",
    "                    done = False\n",
    "                    reward = -1\n",
    "                    return done, reward\n",
    "        #         np.where([True if self.GRID[(self._box_location + val)[0], (self._box_location + val)[1] ] == 1 else False for val in self._action_to_direction.values() ] == True)[0][0]: # gotta check if the box is not adjacent to 2 walls but still is terminating state like the boundary walls\n",
    "        else:\n",
    "            done = False\n",
    "            reward = -1\n",
    "        return done, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8213b62-f81b-4fbd-ad1d-a95f6dca48ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "env  = WarehouseAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5146d19e-6162-4b10-bded-ee8918de3c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8080af2d-67a3-4823-b507-c956004425b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ep_soft(env):\n",
    "    policy = np.ones([env.state_space.n,env.action_space.n])/5\n",
    "    return policy\n",
    "    \n",
    "def gen_epsiode(env,policy):\n",
    "    env.reset()\n",
    "    done = False\n",
    "    episode = []\n",
    "    count = 0\n",
    "    while not done and count<100:\n",
    "        s = env._state_in_seq()\n",
    "        prob = policy[s]\n",
    "        action = np.random.choice(range(env.action_space.n),p=prob)\n",
    "        observation, reward, done ,_ = env.step(action)\n",
    "        episode.append([s,action,reward])\n",
    "        count+=1\n",
    "    return episode\n",
    "def visit_to_s(env,episode,Returns):\n",
    "    visited = []\n",
    "    R = None\n",
    "    for ind,s in enumerate(episode):\n",
    "        if s not in visited:\n",
    "            act = s[1]\n",
    "            for p in episode[ind:]:\n",
    "                R+=p[2]\n",
    "            Returns[ind,act] = R\n",
    "        else:\n",
    "            continue\n",
    "    return Returns\n",
    "def Q_val(Returns):\n",
    "    Q = np.average(Returns,axis=2)\n",
    "    return Q\n",
    "            \n",
    "def MC(env,ep = 0.1):\n",
    "    Q = np.zeros([env.state_space.n,env.action_space.n])\n",
    "    Returns = np.zeros([env.state_space.n,env.action_space.n])\n",
    "    policy = ep_soft(env)\n",
    "    for k in range(1):\n",
    "        episode = gen_epsiode(env,policy)\n",
    "        print(len(episode))\n",
    "        Returns  = visit_to_s(env,episode,Returns)\n",
    "        Q = Q_val(Returns)\n",
    "        for ind,step in enumerate(episode):\n",
    "            a_star = np.argmax(Q,axis = 1)\n",
    "            for a in range(env.action_space.n):\n",
    "                if a == a_star:\n",
    "                    policy[step[0],step[a]] = (1 - ep) + (ep/env.action_space.n)\n",
    "                else:\n",
    "                    policy[step[0],step[a]] = (ep/env.action_space.n)\n",
    "            \n",
    "    \n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "69fd7538-f9d2-4056-8a15-1479901ab8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = WarehouseAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9082d421-ab82-4f58-90a0-48200c378e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21580\\299910126.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# policy = ep_soft(env)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# gen_epsiode(env,policy)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mMC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21580\\483898282.py\u001b[0m in \u001b[0;36mMC\u001b[1;34m(env, ep)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mepisode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_epsiode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mReturns\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mvisit_to_s\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mReturns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQ_val\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mReturns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21580\\483898282.py\u001b[0m in \u001b[0;36mvisit_to_s\u001b[1;34m(env, episode, Returns)\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[0mR\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mReturns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "# policy = ep_soft(env)\n",
    "# gen_epsiode(env,policy)\n",
    "MC(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d546ccbe-626b-4591-8c53-b1f36e8bdd48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde8efa7-e5ff-4ad8-8e53-7e552fe6eb50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7da7271-af59-4d73-b2c0-d649e711349a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34262d-e344-4ec4-b9e0-a7c96f9305e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea3510f-1d11-4d2a-b302-2d074151af67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18842437-c504-48f3-95e4-8adcf42c9b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 2,  2],\n",
       "        [ 1,  1]],\n",
       "\n",
       "       [[ 2,  2],\n",
       "        [ 5, 10]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(np.zeros([env.state_space.n,env.action_space.n])+1,axis=1)\n",
    "# np.zeros([env._state_in_seq(),env.action_space.n])+1\n",
    "# env.state_space.n\n",
    "a = np.array([[[2,2],[1,1]],[[2,2],[5,10]]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0da21f61-4940-403c-a279-85a9508bdf2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.average(a,axis=2),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "25bf24eb-c3aa-4194-9431-b50ee73a0d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice([1,2,3,5],p = [0.1,0.5,0.3,0.1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
