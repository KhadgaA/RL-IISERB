{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a81e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import Env\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box, Dict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fbb35b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akash\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "class WarehouseAgent:\n",
    "    def __init__(self):\n",
    "        self.GRID_DIM = [7, 6]\n",
    "\n",
    "        self.agent_position = [1, 2]\n",
    "\n",
    "        self.box_location = [4, 3]\n",
    "        self.goal_location = [3, 1]\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([-1, 0]),\n",
    "            1: np.array([1, 0]),\n",
    "            2: np.array([0, -1]),\n",
    "            3: np.array([0, 1]),\n",
    "        }\n",
    "        self._ACTIONLOOKUP = {\n",
    "            0: \"move up\",\n",
    "            1: \"move down\",\n",
    "            2: \"move left\",\n",
    "            3: \"move right\",\n",
    "            4: \"push\",\n",
    "        }\n",
    "        self.GRID_DIM = np.asarray(self.GRID_DIM)\n",
    "        self.GRID = np.zeros(\n",
    "            self.GRID_DIM\n",
    "        )  # The Boundaries are the walls, so playing space is only [:-2,:-2]\n",
    "        self.GRID[:, [0, -1]] = 1\n",
    "        self.GRID[[0, -1], :] = 1\n",
    "        self.GRID[[1, 2, 5], 3:5] = 1\n",
    "        self.walls = 1\n",
    "        self.action_space = Discrete(len(self._ACTIONLOOKUP.keys()))\n",
    "        self.state_space = Discrete(self.GRID_DIM[0] * self.GRID_DIM[1])\n",
    "        self.observation_space = Dict(\n",
    "            {\n",
    "                \"agent\": Box(\n",
    "                    np.array([0, 0]),\n",
    "                    np.array([self.GRID_DIM[0] - 1, self.GRID_DIM[1] - 1]),\n",
    "                    shape=(2,),\n",
    "                    dtype=int,\n",
    "                ),\n",
    "                \"box\": Box(\n",
    "                    np.array([0, 0]),\n",
    "                    np.array([self.GRID_DIM[0] - 1, self.GRID_DIM[1] - 1]),\n",
    "                    shape=(2,),\n",
    "                    dtype=int,\n",
    "                ),\n",
    "                \"target\": Box(\n",
    "                    np.array([0, 0]),\n",
    "                    np.array([self.GRID_DIM[0] - 1, self.GRID_DIM[1] - 1]),\n",
    "                    shape=(2,),\n",
    "                    dtype=int,\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        self._agent_location = np.array(self.agent_position)\n",
    "        self._box_location = np.array(self.box_location)\n",
    "        self._target_location = np.array(self.goal_location)\n",
    "\n",
    "    #         print(self.GRID)\n",
    "\n",
    "    def step(self, action):\n",
    "        self._prev_agent_location = None\n",
    "        self._prev_box_location = None\n",
    "        moved_box = False\n",
    "\n",
    "        if action < 4:\n",
    "            moved_player = self._move(action)\n",
    "        else:\n",
    "            moved_player, moved_box = self._push(action)\n",
    "\n",
    "        done, reward = self.is_over()\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        rend = self.GRID.copy().astype(dtype=\"U1\")\n",
    "        rend[self._agent_location[0], self._agent_location[1]] = \"A\"\n",
    "        rend[self._box_location[0], self._box_location[1]] = \"B\"\n",
    "        rend[self._target_location[0], self._target_location[1]] = \"T\"\n",
    "        if np.array_equal(self._target_location, self._box_location):\n",
    "            rend[self._target_location[0], self._target_location[1]] = \"D\"\n",
    "        return print(rend)\n",
    "\n",
    "    def reset(self, seed=None, return_info=False, options=None):\n",
    "        self._agent_location = np.array(self.agent_position)\n",
    "        self._box_location = np.array(self.box_location)\n",
    "        self._target_location = np.array(self.goal_location)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        return (observation, info) if return_info else observation\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "            \"agent\": self._agent_location,\n",
    "            \"box\": self._box_location,\n",
    "            \"target\": self._target_location,\n",
    "        }\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self._box_location - self._target_location, ord=1\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def _state_in_seq(self):\n",
    "        m, n = self._agent_location\n",
    "        seq = m * self.GRID.shape[1] + n\n",
    "        return seq\n",
    "\n",
    "    def _push(self, action):\n",
    "        loc = self._box_location - self._agent_location\n",
    "        #         print(f'loc{loc}, box :{self._box_location}, agent:{self._agent_location}')\n",
    "        push_dir = None\n",
    "        for idx, val in enumerate(self._action_to_direction.values()):\n",
    "            if np.array_equal(loc, val):\n",
    "                valid = True\n",
    "                push_dir = idx\n",
    "                break\n",
    "            else:\n",
    "                valid = False\n",
    "\n",
    "        if valid:\n",
    "            self._prev_agent_location = self._agent_location\n",
    "            self._prev_box_location = self._box_location\n",
    "            self._box_location = (\n",
    "                self._box_location + self._action_to_direction[push_dir]\n",
    "            )\n",
    "            if self.GRID[self._box_location[0], self._box_location[1]] == 1:\n",
    "                self._box_location = self._prev_box_location\n",
    "                return False, False\n",
    "            else:\n",
    "                self._agent_location = (\n",
    "                    self._agent_location + self._action_to_direction[push_dir]\n",
    "                )\n",
    "                return True, True\n",
    "\n",
    "        return False, False\n",
    "\n",
    "    def _move(self, action):\n",
    "        self._prev_agent_location = self._agent_location\n",
    "        self._prev_box_location = self._box_location\n",
    "        self._agent_location = self._agent_location + self._action_to_direction[action]\n",
    "        #             print(self.GRID[self._agent_location],self._agent_location,self.GRID)\n",
    "        if self.GRID[self._agent_location[0], self._agent_location[1]] == 1:\n",
    "            self._agent_location = self._prev_agent_location\n",
    "            return False\n",
    "        elif np.array_equal(self._agent_location, self._box_location):\n",
    "            self._agent_location = self._prev_agent_location\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def is_over(self):\n",
    "        if np.array_equal(\n",
    "            self._box_location, self._target_location\n",
    "        ):  # checking if the box is at the target already\n",
    "            done = True\n",
    "            reward = 0\n",
    "        elif (\n",
    "            sum(\n",
    "                a := np.array(\n",
    "                    [\n",
    "                        True\n",
    "                        if self.GRID[\n",
    "                            (self._box_location + val)[0], (self._box_location + val)[1]\n",
    "                        ]\n",
    "                        == 1\n",
    "                        else False\n",
    "                        for val in self._action_to_direction.values()\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            >= 1\n",
    "        ):\n",
    "            # basically checking if there are atleast 1 wall adjacent to box\n",
    "            if sum(a) > 1:\n",
    "                done = True\n",
    "                reward = -1\n",
    "            elif sum(a) == 1:\n",
    "                if ~(self._box_location - self._target_location).all():\n",
    "                    done = False\n",
    "                    reward = -1\n",
    "                    return done, reward\n",
    "                else:\n",
    "                    #                 print(a)\n",
    "                    direc = np.where(a == True)\n",
    "                    #                 print(direc)\n",
    "                    direc = direc[0][0]\n",
    "                    left = self._box_location + self._action_to_direction[direc]\n",
    "                    right = left.copy()\n",
    "                    if direc in [0, 1]:\n",
    "                        count = 0\n",
    "                        while (self.GRID[left[0], left[1]] != 0) and (\n",
    "                            self.GRID[right[0], right[1]] != 0\n",
    "                        ):\n",
    "\n",
    "                            left = np.clip(\n",
    "                                left + self._action_to_direction[2],\n",
    "                                [0, 0],\n",
    "                                [self.GRID_DIM[0] - 1, self.GRID_DIM[1] - 1],\n",
    "                            )\n",
    "                            right = np.clip(\n",
    "                                right + self._action_to_direction[3],\n",
    "                                [0, 0],\n",
    "                                [self.GRID_DIM[0] - 1, self.GRID_DIM[1] - 1],\n",
    "                            )\n",
    "                            count += 1\n",
    "                            if count >= self.GRID_DIM[1]:\n",
    "                                done = True\n",
    "                                reward = -1\n",
    "                                return done, reward\n",
    "                                break\n",
    "                    #                         right = right + self._action_to_direction[3]\n",
    "\n",
    "                    else:\n",
    "                        count = 0\n",
    "                        while (self.GRID[left[0], left[1]] != 0) and (\n",
    "                            self.GRID[right[0], right[1]] != 0\n",
    "                        ):\n",
    "                            left = np.clip(\n",
    "                                left + self._action_to_direction[1],\n",
    "                                [0, 0],\n",
    "                                [self.GRID_DIM[0] - 1, self.GRID_DIM[1] - 1],\n",
    "                            )\n",
    "                            right = np.clip(\n",
    "                                right + self._action_to_direction[0],\n",
    "                                [0, 0],\n",
    "                                [self.GRID_DIM[0] - 1, self.GRID_DIM[1] - 1],\n",
    "                            )\n",
    "                            count += 1\n",
    "                            if count >= self.GRID_DIM[0]:\n",
    "                                done = True\n",
    "                                reward = -1\n",
    "                                return done, reward\n",
    "                                break\n",
    "\n",
    "                    done = False\n",
    "                    reward = -1\n",
    "                    return done, reward\n",
    "        #         np.where([True if self.GRID[(self._box_location + val)[0], (self._box_location + val)[1] ] == 1 else False for val in self._action_to_direction.values() ] == True)[0][0]: # gotta check if the box is not adjacent to 2 walls but still is terminating state like the boundary walls\n",
    "        else:\n",
    "            done = False\n",
    "            reward = -1\n",
    "        return done, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c45255b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = WarehouseAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c40267cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions = ['a1','a2','a3','a4']     # 4 actions\n",
    "# states = np.array([[1,2,3,4,5,6],  # 6by6 grid  => 36 states\n",
    "#                  [7,8,9,10,11,12],\n",
    "#                  [13,14,15,16,17,18],\n",
    "#                  [19,20,21,22,23,24],\n",
    "#                  [25,26,27,28,29,30],\n",
    "#                  [31,32,33,34,35,36]])\n",
    "\n",
    "\n",
    "# #total_actions = len(actions)\n",
    "# states_size = states.shape[0]*states.shape[1]\n",
    "# print(total_actions,states_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9b9ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the hyper parameters\n",
    "alpha = 0.8\n",
    "gamma = 0.9\n",
    "epsilon = 0.9\n",
    "total_episodes = 10\n",
    "max_steps = 10000\n",
    "\n",
    "#Initializing the Q-table with 0\n",
    "Q = np.ones((env.state_space.n,env.action_space.n))  # (total no. of states * total no. of actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "463d2251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to choose the next action from a state\n",
    "def choose_action(state):\n",
    "    action=0\n",
    "    # epsilon greedy\n",
    "    p = np.random.random()\n",
    "    if p < epsilon:  # choose random action among 4 actions\n",
    "        action =  np.random.randint(Q.shape[1]) # action index           \n",
    "    else:\n",
    "        action = np.argmax(Q[state, :])  # index of maximum action-state value for a state\n",
    "    \n",
    "    return action\n",
    "    \n",
    "# Function to update the Q-table\n",
    "def Qupdate(initial_state, initial_action, next_state, reward, next_action,Q):\n",
    "    Q[initial_state, initial_action] = Q[initial_state, initial_action] + alpha*(reward + (gamma * Q[next_state, next_action])- Q[initial_state, initial_action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bc2d2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the reward\n",
    "episodes_reward = []  # total sum of rewards in a episode, for each episodes\n",
    "\n",
    "# Starting the SARSA learning\n",
    "for episode in range(total_episodes):\n",
    "    #env.reset()\n",
    "    initial_state = env._state_in_seq()    # initial state i.e reset to intial state for each episode  # env.reset()\n",
    "    initial_action = choose_action(initial_state)  # initial action for this initial_state\n",
    "    total_reward = 0\n",
    "    \n",
    "    #done = False        # done is for terminated or not\n",
    "    step = 0\n",
    "    \n",
    "    # loop in the episode until the environment not terminated\n",
    "    while step < max_steps:   # if we put limit on no. of steps instead of termination or not  while not done and \n",
    "        \n",
    "        # Getting the next state,reward\n",
    "        observation, reward, done, info = env.step(initial_action)   # Take one step in the environment\n",
    "        next_state = env._state_in_seq()\n",
    "\n",
    "        # Choosing the next action\n",
    "        next_action = choose_action(next_state)\n",
    "        \n",
    "        # Q table update\n",
    "        Qupdate(initial_state, initial_action, next_state, reward, next_action,Q)\n",
    "\n",
    "        initial_state = next_state\n",
    "        initial_action = next_action\n",
    "        \n",
    "        total_reward += reward\n",
    "        step = step+1\n",
    "        \n",
    "    # At the end of learning process i.e termination\n",
    "    episodes_reward.append(total_reward)        \n",
    "    #print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34d1db4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf08eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8db21193",
   "metadata": {},
   "outputs": [],
   "source": [
    "act = np.argmax(Q,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2385c74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0\n",
      " 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "803232ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01da3773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beb203c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._state_in_seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1fe837e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 2]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 1]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 1]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 1]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 1]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 1]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 1]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 1]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 1]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 1]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 1]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n",
      "({'agent': array([1, 1]), 'box': array([4, 3]), 'target': array([3, 1])}, -1, False, {'distance': 3.0})\n"
     ]
    }
   ],
   "source": [
    "for a in act[8:]:\n",
    "    print(env.step(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51d554e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [-10.        , -10.        , -10.        , -10.        ,\n",
       "        -10.        ],\n",
       "       [-10.        , -10.        , -10.        , -10.        ,\n",
       "        -10.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [-10.        , -10.        , -10.        , -10.        ,\n",
       "        -10.        ],\n",
       "       [-10.        , -10.        , -10.        , -10.        ,\n",
       "        -10.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [-10.        , -10.        , -10.        , -10.        ,\n",
       "        -10.        ],\n",
       "       [-10.        , -10.        , -10.        , -10.        ,\n",
       "        -10.        ],\n",
       "       [-10.        , -10.        , -10.        , -10.        ,\n",
       "        -10.        ],\n",
       "       [-10.        , -10.        , -10.        , -10.        ,\n",
       "        -10.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [-10.        , -10.        , -10.        , -10.        ,\n",
       "        -10.        ],\n",
       "       [-10.        , -10.        , -10.        , -10.        ,\n",
       "        -10.        ],\n",
       "       [-10.        , -10.        , -10.        , -10.        ,\n",
       "        -10.        ],\n",
       "       [-10.        , -10.        , -10.        , -10.        ,\n",
       "        -10.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [ -1.01684173,   0.12      ,   1.        ,  -0.94304   ,\n",
       "         -0.63002065],\n",
       "       [-10.        , -10.        , -10.        , -10.        ,\n",
       "        -10.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
       "          1.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa74de19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
